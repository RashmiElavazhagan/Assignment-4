#-------------------------------------------------------------------------
# AUTHOR: Rashmi Elavazhagan
# FILENAME: bagging_random_forest
# SPECIFICATION: Developing and evaluating the effectiveness of bagging and random forest algorithms.
# FOR: CS 5990- Assignment #4
# TIME SPENT: 3 hours
#-----------------------------------------------------------*/

#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.preprocessing import LabelEncoder

training_data = []
test_data = []
X_training = []
y_training = []
class_votes = [] #this array will be used to count the votes of each classifier

#reading the training data from a csv file and populate training_data
#--> add your Python code here
training_data = pd.read_csv('optdigits.tra', sep=',', header=None)
#reading the test data from a csv file and populate test_data
#--> add your Python code here
test_data = pd.read_csv('optdigits.tes', sep=',', header=None).to_numpy()
#inititalizing the class votes for each test sample. Example: class_votes.append([0,0,0,0,0,0,0,0,0,0])
#--> add your Python code here
num_test_data = len(test_data)
for i in range(num_test_data):
    class_votes.append([0,0,0,0,0,0,0,0,0,0])

print("Started my base and ensemble classifier ...")

for k in range(20): #we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample

    bootstrap_sample = resample(training_data, n_samples=len(training_data), replace=True)

    #populate the values of X_training and y_training by using the bootstrap_sample
    X_training = bootstrap_sample.iloc[:,:-1]
    y_training = bootstrap_sample.iloc[:,-1]
  
    #fitting the decision tree to the data
    clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=None) #we will use a single decision tree without pruning it
    clf = clf.fit(X_training, y_training)

    single_test_prediction = 0
    for i, test_sample in enumerate(test_data):
        X_test_data = [test_sample[:-1]]
        prediction = clf.predict(X_test_data)[0]
        class_votes[i][prediction] += 1
        
        if k == 0: #for only the first base classifier,
            #compare the prediction with the true label of the test sample here to start calculating its accuracy
            y_test_data = test_sample[-1]
            if prediction == y_test_data:
                single_test_prediction += 1
                
    if k == 0: #for only the first base classifier, print its accuracy here
        accuracy = single_test_prediction/len(test_data)
        print("Finished my base classifier (fast but relatively low accuracy) ...")
        print("My base classifier accuracy: " + str(accuracy))
        print("")
        
    all_test_predictions = 0
    for i, test_sample in enumerate(test_data):
        y_test_data = test_sample[-1]
        max_class_votes = class_votes[i].index(max(class_votes[i]))
        if y_test_data == max_class_votes:
            all_test_predictions += 1
    accuracy = all_test_predictions/len(test_data)
    print("Finished my ensemble classifier (slow but higher accuracy) ...")
    print("My ensemble accuracy: " + str(accuracy))
    print("")

    print("Started Random Forest algorithm ...")

    #Create a Random Forest Classifier
    clf = RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

    #Fit Random Forest to the training data
    clf.fit(X_training,y_training)

    n_test_data_rf = 0
    for i, test_sample in enumerate(test_data):
        X_test_data = [test_sample[:-1]]
        y_test_data = test_sample[-1]
        prediction = clf.predict(X_test_data)[0]
        if y_test_data == prediction:
            n_test_data_rf += 1
    accuracy = n_test_data_rf/len(test_data)
    print("Random Forest accuracy: " + str(accuracy))

    print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
